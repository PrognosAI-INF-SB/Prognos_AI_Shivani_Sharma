{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000b97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"processed\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at models_m2\\scaler_fd1_milestone4.save\n",
      " Data ready: X_train=(1128, 30, 2), X_val=(282, 30, 2)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 30, 128)           67072     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 30, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119361 (466.25 KB)\n",
      "Trainable params: 118977 (464.75 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 22s 424ms/step - loss: 30.8133 - mae: 4.9651 - val_loss: 72.2121 - val_mae: 8.3898\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 5s 259ms/step - loss: 8.4775 - mae: 2.4216 - val_loss: 60.1383 - val_mae: 7.6137\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 3s 185ms/step - loss: 4.4617 - mae: 1.6888 - val_loss: 54.4258 - val_mae: 7.2283\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 4s 207ms/step - loss: 3.7197 - mae: 1.5448 - val_loss: 38.9408 - val_mae: 6.0399\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 4s 212ms/step - loss: 3.3521 - mae: 1.4809 - val_loss: 31.9429 - val_mae: 5.4383\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 3s 155ms/step - loss: 3.4081 - mae: 1.4754 - val_loss: 25.8016 - val_mae: 4.8616\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 3s 141ms/step - loss: 3.1615 - mae: 1.4353 - val_loss: 23.6864 - val_mae: 4.6347\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 3s 142ms/step - loss: 3.3230 - mae: 1.4586 - val_loss: 23.3256 - val_mae: 4.5806\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 3s 187ms/step - loss: 2.8093 - mae: 1.3443 - val_loss: 22.0640 - val_mae: 4.4317\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 3s 187ms/step - loss: 2.6481 - mae: 1.3175 - val_loss: 14.9404 - val_mae: 3.5277\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 4s 248ms/step - loss: 2.8256 - mae: 1.3416 - val_loss: 12.6789 - val_mae: 3.1721\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 3s 195ms/step - loss: 3.0773 - mae: 1.4051 - val_loss: 7.9838 - val_mae: 2.4217\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 2.4525 - mae: 1.2548 - val_loss: 6.4330 - val_mae: 2.1398\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 2s 132ms/step - loss: 2.4004 - mae: 1.2385 - val_loss: 2.7257 - val_mae: 1.3557\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 6s 319ms/step - loss: 2.4596 - mae: 1.2513 - val_loss: 2.8616 - val_mae: 1.3877\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 7s 369ms/step - loss: 2.2568 - mae: 1.2145 - val_loss: 3.0803 - val_mae: 1.4390\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 6s 335ms/step - loss: 2.3071 - mae: 1.2260 - val_loss: 2.9492 - val_mae: 1.3985\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 6s 321ms/step - loss: 2.3305 - mae: 1.2315 - val_loss: 2.4038 - val_mae: 1.2713\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 5s 282ms/step - loss: 2.7063 - mae: 1.3197 - val_loss: 2.5340 - val_mae: 1.3033\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 4s 210ms/step - loss: 2.5331 - mae: 1.2709 - val_loss: 2.5505 - val_mae: 1.2908\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 4s 231ms/step - loss: 2.4466 - mae: 1.2619 - val_loss: 2.2507 - val_mae: 1.2165\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 4s 236ms/step - loss: 2.4083 - mae: 1.2637 - val_loss: 2.4343 - val_mae: 1.2604\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 5s 306ms/step - loss: 2.2739 - mae: 1.2204 - val_loss: 2.0703 - val_mae: 1.1545\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 8s 448ms/step - loss: 2.2105 - mae: 1.2035 - val_loss: 3.1900 - val_mae: 1.4178\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 6s 356ms/step - loss: 2.3761 - mae: 1.2439 - val_loss: 2.9559 - val_mae: 1.3706\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 7s 404ms/step - loss: 2.3281 - mae: 1.2297 - val_loss: 2.1206 - val_mae: 1.1742\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 6s 362ms/step - loss: 2.2453 - mae: 1.2081 - val_loss: 2.0460 - val_mae: 1.1417\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 6s 354ms/step - loss: 2.1790 - mae: 1.1906 - val_loss: 2.3556 - val_mae: 1.2024\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 14s 800ms/step - loss: 2.0840 - mae: 1.1634 - val_loss: 2.3167 - val_mae: 1.2267\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 7s 385ms/step - loss: 2.1717 - mae: 1.1872 - val_loss: 2.5287 - val_mae: 1.2591\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 6s 311ms/step - loss: 1.8995 - mae: 1.1279 - val_loss: 2.8750 - val_mae: 1.3483\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 8s 446ms/step - loss: 2.0427 - mae: 1.1483 - val_loss: 3.4059 - val_mae: 1.4851\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 7s 359ms/step - loss: 1.9138 - mae: 1.1178 - val_loss: 2.5591 - val_mae: 1.2527\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 4s 245ms/step - loss: 2.0773 - mae: 1.1688 - val_loss: 2.3536 - val_mae: 1.2229\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 5s 309ms/step - loss: 2.0763 - mae: 1.1527 - val_loss: 2.0097 - val_mae: 1.1462\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 9s 513ms/step - loss: 2.2083 - mae: 1.1925 - val_loss: 2.0749 - val_mae: 1.1932\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 9s 519ms/step - loss: 2.0822 - mae: 1.1593 - val_loss: 1.7462 - val_mae: 1.0681\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 6s 360ms/step - loss: 2.0121 - mae: 1.1390 - val_loss: 1.6180 - val_mae: 1.0532\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 8s 469ms/step - loss: 2.0745 - mae: 1.1480 - val_loss: 1.6253 - val_mae: 1.0388\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 8s 423ms/step - loss: 2.1866 - mae: 1.1733 - val_loss: 1.9958 - val_mae: 1.1103\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 6s 336ms/step - loss: 2.1990 - mae: 1.2051 - val_loss: 1.7600 - val_mae: 1.0918\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 4s 247ms/step - loss: 2.0089 - mae: 1.1314 - val_loss: 1.6978 - val_mae: 1.0601\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 4s 217ms/step - loss: 1.9716 - mae: 1.1205 - val_loss: 1.6434 - val_mae: 1.0567\n"
     ]
    }
   ],
   "source": [
    "# milestone4_full_ready.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import joblib\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "fd_number = 1\n",
    "window_size = 30\n",
    "data_file = \"sensor_data.csv\"\n",
    "\n",
    "processed_dir = \"processed\"\n",
    "model_dir = \"models_m2\"\n",
    "results_dir = \"results\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, f\"optimized_fd{fd_number}_milestone4.h5\")\n",
    "scaler_path = os.path.join(model_dir, f\"scaler_fd{fd_number}_milestone4.save\")\n",
    "\n",
    "# ------------------ LOAD CSV ------------------\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Keep only numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "df.fillna(method='ffill', inplace=True)  # forward fill NaN\n",
    "\n",
    "# Features and target\n",
    "X = df.iloc[:, :-1].values.astype(float)\n",
    "y = df.iloc[:, -1].values.astype(float)\n",
    "\n",
    "# ------------------ SCALE ------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# ------------------ CREATE SLIDING WINDOWS ------------------\n",
    "X_lstm = []\n",
    "y_lstm = []\n",
    "\n",
    "for i in range(len(X_scaled) - window_size):\n",
    "    X_lstm.append(X_scaled[i:i+window_size, :])\n",
    "    y_lstm.append(y[i+window_size-1])\n",
    "\n",
    "X_lstm = np.array(X_lstm)\n",
    "y_lstm = np.array(y_lstm)\n",
    "\n",
    "# Split into train/validation (80%-20%)\n",
    "split_idx = int(0.8 * len(X_lstm))\n",
    "X_train, y_train = X_lstm[:split_idx], y_lstm[:split_idx]\n",
    "X_val, y_val = X_lstm[split_idx:], y_lstm[split_idx:]\n",
    "\n",
    "print(f\" Data ready: X_train={X_train.shape}, X_val={X_val.shape}\")\n",
    "\n",
    "# ------------------ BUILD LSTM MODEL ------------------\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    LSTM(64, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# ------------------ CALLBACKS ------------------\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# ------------------ TRAIN MODEL ------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\" Model saved at {model_path}\")\n",
    "\n",
    "# ------------------ EVALUATE ------------------\n",
    "y_val_pred = model.predict(X_val).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "accuracy = 100 - (rmse / np.max(y_val) * 100)\n",
    "\n",
    "print(f\"\\nModel Evaluation — FD00{fd_number}\")\n",
    "print(f\"RMSE : {rmse:.4f}\")\n",
    "print(f\"MAE  : {mae:.4f}\")\n",
    "print(f\"R² Score : {r2:.4f}\")\n",
    "print(f\"Approx. Accuracy : {accuracy:.2f}%\")\n",
    "\n",
    "# ------------------ SAVE METRICS ------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"FD_Set\": [f\"FD00{fd_number}\"],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2_Score\": [r2],\n",
    "    \"Accuracy(%)\": [accuracy]\n",
    "})\n",
    "results_df.to_csv(os.path.join(results_dir, f\"model_performance_fd{fd_number}_milestone4.csv\"), index=False)\n",
    "print(f\"Metrics saved to {results_dir}\")\n",
    "\n",
    "# ------------------ ALERTS ------------------\n",
    "warning_threshold = 50\n",
    "critical_threshold = 20\n",
    "\n",
    "alerts = []\n",
    "for i, rul in enumerate(y_val_pred):\n",
    "    if rul <= critical_threshold:\n",
    "        alerts.append((i, rul, \"CRITICAL\"))\n",
    "    elif rul <= warning_threshold:\n",
    "        alerts.append((i, rul, \"WARNING\"))\n",
    "\n",
    "alert_df = pd.DataFrame(alerts, columns=[\"Sample_Index\", \"Predicted_RUL\", \"Alert_Level\"])\n",
    "alert_df.to_csv(os.path.join(results_dir, f\"alerts_fd{fd_number}_milestone4.csv\"), index=False)\n",
    "print(f\"Alerts saved to {results_dir}\")\n",
    "print(alert_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131efb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
