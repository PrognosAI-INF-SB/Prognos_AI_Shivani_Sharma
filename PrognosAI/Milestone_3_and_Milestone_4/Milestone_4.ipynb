{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000b97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"processed\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4d78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved at models_m2\\scaler_fd1_milestone4.save\n",
      " Data ready: X_train=(1128, 30, 2), X_val=(282, 30, 2)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 30, 128)           67072     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 30, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119361 (466.25 KB)\n",
      "Trainable params: 118977 (464.75 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 15s 210ms/step - loss: 33.2033 - mae: 5.3248 - val_loss: 64.1610 - val_mae: 7.8907\n",
      "Epoch 2/100\n",
      " 1/18 [>.............................] - ETA: 1s - loss: 15.0707 - mae: 3.3528"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 93ms/step - loss: 8.0882 - mae: 2.3650 - val_loss: 41.0194 - val_mae: 6.2301\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 2s 93ms/step - loss: 3.7892 - mae: 1.5332 - val_loss: 26.8414 - val_mae: 4.9840\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 2s 95ms/step - loss: 3.6309 - mae: 1.5142 - val_loss: 26.3523 - val_mae: 4.9375\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 2s 98ms/step - loss: 3.4620 - mae: 1.4689 - val_loss: 31.2928 - val_mae: 5.3990\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 2s 90ms/step - loss: 3.3921 - mae: 1.4861 - val_loss: 29.8065 - val_mae: 5.2574\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 2s 90ms/step - loss: 3.1756 - mae: 1.4148 - val_loss: 24.4920 - val_mae: 4.7371\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 3.3662 - mae: 1.4715 - val_loss: 20.4531 - val_mae: 4.2822\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 2s 91ms/step - loss: 2.7510 - mae: 1.3301 - val_loss: 15.9168 - val_mae: 3.7095\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 2.7165 - mae: 1.3132 - val_loss: 14.8558 - val_mae: 3.5501\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 2s 91ms/step - loss: 2.5998 - mae: 1.2740 - val_loss: 10.8383 - val_mae: 2.9352\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 3.1081 - mae: 1.4092 - val_loss: 10.2737 - val_mae: 2.8094\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 2.6432 - mae: 1.2894 - val_loss: 6.3471 - val_mae: 2.1417\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 2.3215 - mae: 1.2173 - val_loss: 5.5376 - val_mae: 1.9691\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 2s 94ms/step - loss: 2.3381 - mae: 1.2402 - val_loss: 3.1991 - val_mae: 1.4789\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 2s 90ms/step - loss: 2.5162 - mae: 1.2725 - val_loss: 3.8894 - val_mae: 1.6270\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 2.8710 - mae: 1.3559 - val_loss: 2.2000 - val_mae: 1.2318\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 2.5559 - mae: 1.2631 - val_loss: 4.2231 - val_mae: 1.7016\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.4805 - mae: 1.2669 - val_loss: 4.0074 - val_mae: 1.6446\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 2.9840 - mae: 1.3719 - val_loss: 2.1226 - val_mae: 1.2027\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.2586 - mae: 1.2065 - val_loss: 2.2955 - val_mae: 1.2357\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 2.4133 - mae: 1.2467 - val_loss: 3.1138 - val_mae: 1.4290\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 2s 92ms/step - loss: 2.1334 - mae: 1.1795 - val_loss: 2.0786 - val_mae: 1.1854\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 2.4067 - mae: 1.2612 - val_loss: 2.1385 - val_mae: 1.1855\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 2s 91ms/step - loss: 2.3342 - mae: 1.2302 - val_loss: 2.0685 - val_mae: 1.1566\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 2s 90ms/step - loss: 2.3000 - mae: 1.1915 - val_loss: 2.2140 - val_mae: 1.2084\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 2.1937 - mae: 1.1989 - val_loss: 2.1966 - val_mae: 1.1760\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 2.1836 - mae: 1.1843 - val_loss: 2.4646 - val_mae: 1.2417\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.1762 - mae: 1.1898 - val_loss: 2.8714 - val_mae: 1.3421\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.2320 - mae: 1.1955 - val_loss: 2.3119 - val_mae: 1.2092\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 2s 99ms/step - loss: 2.1012 - mae: 1.1610 - val_loss: 1.7728 - val_mae: 1.0991\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 2s 95ms/step - loss: 2.3812 - mae: 1.2290 - val_loss: 1.9328 - val_mae: 1.1157\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 2s 97ms/step - loss: 2.1272 - mae: 1.1718 - val_loss: 2.2716 - val_mae: 1.2040\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.2800 - mae: 1.1992 - val_loss: 2.2555 - val_mae: 1.1729\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.2786 - mae: 1.2013 - val_loss: 2.2492 - val_mae: 1.1904\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 2s 94ms/step - loss: 2.0261 - mae: 1.1368 - val_loss: 2.1838 - val_mae: 1.1577\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 2s 96ms/step - loss: 1.9719 - mae: 1.1336 - val_loss: 1.7476 - val_mae: 1.0819\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 2s 95ms/step - loss: 1.9753 - mae: 1.1247 - val_loss: 1.9565 - val_mae: 1.1140\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.0883 - mae: 1.1592 - val_loss: 2.3482 - val_mae: 1.1991\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.0557 - mae: 1.1294 - val_loss: 2.2129 - val_mae: 1.1639\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.2786 - mae: 1.2071 - val_loss: 2.1382 - val_mae: 1.1458\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 2s 98ms/step - loss: 2.0062 - mae: 1.1273 - val_loss: 2.3307 - val_mae: 1.1933\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 1.9406 - mae: 1.1172 - val_loss: 2.1916 - val_mae: 1.1574\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 2s 88ms/step - loss: 2.1274 - mae: 1.1656 - val_loss: 2.2126 - val_mae: 1.1630\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 1.9608 - mae: 1.1262 - val_loss: 2.5852 - val_mae: 1.2648\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 2.0556 - mae: 1.1410 - val_loss: 2.1691 - val_mae: 1.1530\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 2s 89ms/step - loss: 1.9727 - mae: 1.1048 - val_loss: 2.3339 - val_mae: 1.1951\n",
      " Model saved at models_m2\\optimized_fd1_milestone4.h5\n",
      "9/9 [==============================] - 2s 33ms/step\n",
      "\n",
      "Model Evaluation — FD001\n",
      "RMSE : 1.3220\n",
      "MAE  : 1.0819\n",
      "R² Score : 0.0146\n",
      "Approx. Accuracy : 89.53%\n",
      "Metrics saved to results\n",
      "Alerts saved to results\n",
      "   Sample_Index  Predicted_RUL Alert_Level\n",
      "0             0       9.232335    CRITICAL\n",
      "1             1       9.210891    CRITICAL\n",
      "2             2       9.084116    CRITICAL\n",
      "3             3       9.017101    CRITICAL\n",
      "4             4       9.000629    CRITICAL\n",
      "5             5       8.949480    CRITICAL\n",
      "6             6       8.962654    CRITICAL\n",
      "7             7       9.000751    CRITICAL\n",
      "8             8       9.091536    CRITICAL\n",
      "9             9       9.103357    CRITICAL\n"
     ]
    }
   ],
   "source": [
    "# milestone4_full_ready.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import joblib\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "fd_number = 1\n",
    "window_size = 30\n",
    "data_file = \"sensor_data.csv\"\n",
    "\n",
    "processed_dir = \"processed\"\n",
    "model_dir = \"models_m2\"\n",
    "results_dir = \"results\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, f\"optimized_fd{fd_number}_milestone4.h5\")\n",
    "scaler_path = os.path.join(model_dir, f\"scaler_fd{fd_number}_milestone4.save\")\n",
    "\n",
    "# ------------------ LOAD CSV ------------------\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Keep only numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "df.fillna(method='ffill', inplace=True)  # forward fill NaN\n",
    "\n",
    "# Features and target\n",
    "X = df.iloc[:, :-1].values.astype(float)\n",
    "y = df.iloc[:, -1].values.astype(float)\n",
    "\n",
    "# ------------------ SCALE ------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved at {scaler_path}\")\n",
    "\n",
    "# ------------------ CREATE SLIDING WINDOWS ------------------\n",
    "X_lstm = []\n",
    "y_lstm = []\n",
    "\n",
    "for i in range(len(X_scaled) - window_size):\n",
    "    X_lstm.append(X_scaled[i:i+window_size, :])\n",
    "    y_lstm.append(y[i+window_size-1])\n",
    "\n",
    "X_lstm = np.array(X_lstm)\n",
    "y_lstm = np.array(y_lstm)\n",
    "\n",
    "# Split into train/validation (80%-20%)\n",
    "split_idx = int(0.8 * len(X_lstm))\n",
    "X_train, y_train = X_lstm[:split_idx], y_lstm[:split_idx]\n",
    "X_val, y_val = X_lstm[split_idx:], y_lstm[split_idx:]\n",
    "\n",
    "print(f\" Data ready: X_train={X_train.shape}, X_val={X_val.shape}\")\n",
    "\n",
    "# ------------------ BUILD LSTM MODEL ------------------\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    LSTM(64, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# ------------------ CALLBACKS ------------------\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# ------------------ TRAIN MODEL ------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\" Model saved at {model_path}\")\n",
    "\n",
    "# ------------------ EVALUATE ------------------\n",
    "y_val_pred = model.predict(X_val).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "accuracy = 100 - (rmse / np.max(y_val) * 100)\n",
    "\n",
    "print(f\"\\nModel Evaluation — FD00{fd_number}\")\n",
    "print(f\"RMSE : {rmse:.4f}\")\n",
    "print(f\"MAE  : {mae:.4f}\")\n",
    "print(f\"R² Score : {r2:.4f}\")\n",
    "print(f\"Approx. Accuracy : {accuracy:.2f}%\")\n",
    "\n",
    "# ------------------ SAVE METRICS ------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"FD_Set\": [f\"FD00{fd_number}\"],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"MAE\": [mae],\n",
    "    \"R2_Score\": [r2],\n",
    "    \"Accuracy(%)\": [accuracy]\n",
    "})\n",
    "results_df.to_csv(os.path.join(results_dir, f\"model_performance_fd{fd_number}_milestone4.csv\"), index=False)\n",
    "print(f\"Metrics saved to {results_dir}\")\n",
    "\n",
    "# ------------------ ALERTS ------------------\n",
    "warning_threshold = 50\n",
    "critical_threshold = 20\n",
    "\n",
    "alerts = []\n",
    "for i, rul in enumerate(y_val_pred):\n",
    "    if rul <= critical_threshold:\n",
    "        alerts.append((i, rul, \"CRITICAL\"))\n",
    "    elif rul <= warning_threshold:\n",
    "        alerts.append((i, rul, \"WARNING\"))\n",
    "\n",
    "alert_df = pd.DataFrame(alerts, columns=[\"Sample_Index\", \"Predicted_RUL\", \"Alert_Level\"])\n",
    "alert_df.to_csv(os.path.join(results_dir, f\"alerts_fd{fd_number}_milestone4.csv\"), index=False)\n",
    "print(f\"Alerts saved to {results_dir}\")\n",
    "print(alert_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131efb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
